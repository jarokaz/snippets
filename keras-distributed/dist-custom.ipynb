{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.conv = layers.Conv1D(32, 3, strides=1, padding='same', activation='relu')\n",
    "        self.rnn = layers.Bidirectional(layers.GRU(32, return_sequences=True))\n",
    "        self.rnn = layers.GRU(32, return_sequences=True)\n",
    "        self.dense = layers.Dense(5, name='signal_mask')\n",
    "        \n",
    "    def call(self, inputs):        \n",
    "        seq_lengths = inputs[1]\n",
    "        sequences = inputs[0]\n",
    "        x = self.conv(sequences)\n",
    "        mask = tf.sequence_mask(seq_lengths)\n",
    "        x = self.rnn(x, mask=mask)\n",
    "        y = self.dense(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "time_step = 100\n",
    "signal_length = 100\n",
    "sequences = np.random.rand(batch_size, time_step, 1)\n",
    "seq_lengths = np.array([signal_length]*batch_size)\n",
    "signal_mask = np.random.randint(5, size=(batch_size, time_step))\n",
    "\n",
    "inputs = tf.data.Dataset.from_tensor_slices((sequences, seq_lengths))\n",
    "signal_masks = tf.data.Dataset.from_tensor_slices(signal_mask)\n",
    "dataset = tf.data.Dataset.zip((inputs, signal_masks))\n",
    "dataset = dataset.repeat(100).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training loss (for one batch) at step 0: 1.6106\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 10: 1.5911\n",
      "Seen so far: 704 samples\n",
      "Training loss (for one batch) at step 20: 1.5880\n",
      "Seen so far: 1344 samples\n",
      "Training loss (for one batch) at step 30: 1.5828\n",
      "Seen so far: 1984 samples\n",
      "Training loss (for one batch) at step 40: 1.5776\n",
      "Seen so far: 2624 samples\n",
      "Training loss (for one batch) at step 50: 1.5707\n",
      "Seen so far: 3264 samples\n",
      "Training loss (for one batch) at step 60: 1.5610\n",
      "Seen so far: 3904 samples\n",
      "Training loss (for one batch) at step 70: 1.5472\n",
      "Seen so far: 4544 samples\n",
      "Training loss (for one batch) at step 80: 1.5273\n",
      "Seen so far: 5184 samples\n",
      "Training loss (for one batch) at step 90: 1.4996\n",
      "Seen so far: 5824 samples\n"
     ]
    }
   ],
   "source": [
    "mymodel = MyModel()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = mymodel(x_batch_train, training=True)\n",
    "            loss_value = loss(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, mymodel.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, mymodel.trainable_weights))\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(((sequences, seq_lengths),  signal_mask)).repeat(10).batch(2)\n",
    "batch, _ = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = layers.Conv1D(32, 3, strides=1, padding='same', activation='relu')\n",
    "#masked = layers.Masking(mask_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = conv(x)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = masked(x1)\n",
    "x2._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(mask_value):\n",
    "    mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "                        \n",
    "    with mirrored_strategy.scope():\n",
    "        seq_data = keras.Input(shape=(None, 1),  name=\"seq_data\")\n",
    "        seq_lengths = keras.Input(shape=(), name=\"seq_lengths\", dtype=tf.int32)\n",
    "\n",
    "        #mask = keras.layers.Lambda(lambda x: tf.sequence_mask(x))(seq_lengths)\n",
    "        conv = layers.Conv1D(32, 3, strides=1, padding='same', activation='relu')(seq_data)\n",
    "        #rnn = layers.Bidirectional(layers.GRU(32, return_sequences=True))(conv,mask=mask)\n",
    "        masked = layers.Masking(mask_value=mask_value)(conv)\n",
    "        rnn = layers.Bidirectional(layers.GRU(32, return_sequences=True))(masked)\n",
    "        dense = layers.Dense(5, name=\"signal_mask\")(rnn)\n",
    "        model = keras.Model(inputs=[seq_data, seq_lengths], outputs=[dense])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001),\n",
    "            loss=tf.keras.backend.sparse_categorical_crossentropy\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model2 = model2(0.0)\n",
    "print(model2.summary())\n",
    "dot_img_file = 'model_2.png'\n",
    "tf.keras.utils.plot_model(model2, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "time_step = 1000\n",
    "signal_length = 1000\n",
    "sequences = np.random.rand(batch_size, time_step, 1)\n",
    "seq_lengths = np.array([signal_length]*batch_size)\n",
    "signal_mask = np.random.randint(5, size=(batch_size, time_step))\n",
    "\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices(((sequences, seq_lengths),  signal_mask)).repeat(10).batch(1)\n",
    "\n",
    "\n",
    "model2.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "time_step = 6\n",
    "signal_length = 6\n",
    "sequences = np.random.rand(batch_size, time_step, 1)\n",
    "seq_lengths = np.array([signal_length]*batch_size)\n",
    "signal_mask = np.random.randint(5, size=(batch_size, time_step))\n",
    "\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices(((sequences, seq_lengths),  signal_mask)).repeat(10).batch(2)\n",
    "batch = next(iter(dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  # Configure a master worker with 1 with K80 GPUs\n",
    "  masterType: n1-highcpu-16\n",
    "  masterConfig:\n",
    "    acceleratorConfig:\n",
    "      count: 2\n",
    "      type: NVIDIA_TESLA_K80\n",
    "  # Configure 1 worker(s), each with 2 K80\n",
    "  workerCount: 1\n",
    "  workerType: n1-highcpu-16\n",
    "  workerConfig:\n",
    "    acceleratorConfig:\n",
    "      count: 2\n",
    "      type: NVIDIA_TESLA_K80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "now=$(date +\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME=\"multi_cpu_fashion_minst_$now\"\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --staging-bucket=gs://mlops-dev-workspace \\\n",
    "  --package-path=sample \\\n",
    "  --module-name=sample.model \\\n",
    "  --runtime-version=2.1 \\\n",
    "  --python-version=3.7 \\\n",
    "  --region=us-west1 \\\n",
    "  --config config.yaml\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs stream-logs multi_cpu_fashion_minst_20200805_040150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
